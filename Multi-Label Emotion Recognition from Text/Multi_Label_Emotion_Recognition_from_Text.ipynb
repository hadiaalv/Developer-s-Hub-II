{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpQ_xNlGumXz"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Vq_OWoOrwjS"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade datasets fsspec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4NCEsINIElH"
      },
      "outputs": [],
      "source": [
        "pip install transformers datasets scikit-learn torch pandas numpy tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMSA1iEiqsyA"
      },
      "outputs": [],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btuBeqBLpno_"
      },
      "source": [
        "# **IMPORTING LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dny0I0pspriK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score, f1_score, hamming_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK1xcZfVp30V"
      },
      "source": [
        "# **LOADING DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrWK3IPIp5xo"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"go_emotions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx8htDOgp9GK"
      },
      "source": [
        "# **INSPECTING DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dE86c2mep_tM"
      },
      "outputs": [],
      "source": [
        "print(dataset['train'][0])  # Inspect a sample from the training data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWN6prFLqBqk"
      },
      "source": [
        "# **TEXT PREPROCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Tbqrr59qFHe"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def preprocess(examples):\n",
        "    # Tokenize\n",
        "    encoding = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "    # Multi-hot encode the labels\n",
        "    num_labels = 28\n",
        "    multi_hot_labels = []\n",
        "    for label_list in examples[\"labels\"]:\n",
        "        vec = [0.0] * num_labels\n",
        "        for label in label_list:\n",
        "            vec[label] = 1.0\n",
        "        multi_hot_labels.append(vec)\n",
        "\n",
        "    encoding[\"labels\"] = multi_hot_labels\n",
        "    return encoding\n",
        "\n",
        "# Apply preprocessing\n",
        "encoded_dataset = dataset.map(preprocess, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8dOcRJK8YJY"
      },
      "outputs": [],
      "source": [
        "from datasets import ClassLabel\n",
        "\n",
        "# Explicitly set label type to float32\n",
        "encoded_dataset.set_format(\n",
        "    type='torch',\n",
        "    columns=['input_ids', 'attention_mask', 'labels'],\n",
        "    output_all_columns=False\n",
        ")\n",
        "\n",
        "# Manually cast labels to float\n",
        "def cast_labels_to_float(batch):\n",
        "    batch[\"labels\"] = batch[\"labels\"].type(torch.float32)\n",
        "    return batch\n",
        "\n",
        "encoded_dataset[\"train\"] = encoded_dataset[\"train\"].map(cast_labels_to_float)\n",
        "encoded_dataset[\"validation\"] = encoded_dataset[\"validation\"].map(cast_labels_to_float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQlkEyn98b_O"
      },
      "outputs": [],
      "source": [
        "print(encoded_dataset['train'][0]['labels'].dtype)  # should print torch.float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC9vG60tqYkc"
      },
      "source": [
        "# **LOADING PRE-TRAINED BERT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwtAaH8cqjbA"
      },
      "outputs": [],
      "source": [
        "# Load Pre-trained BERT for multi-label classification\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=28,\n",
        "    problem_type=\"multi_label_classification\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6F-ivg_qsOU"
      },
      "source": [
        "# **PREPARING DATA COLLATOR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4w40QA4qu8L"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8Q0yRZWqv98"
      },
      "source": [
        "# **DEFINING EVALUATION METRICS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R71dBCu4q0gC"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(p):\n",
        "    predictions = torch.sigmoid(torch.tensor(p.predictions)).numpy()\n",
        "    labels = p.label_ids\n",
        "    return {\n",
        "        'accuracy': accuracy_score(labels, predictions > 0.5),\n",
        "        'f1': f1_score(labels, predictions > 0.5, average='micro'),\n",
        "        'hamming_loss': hamming_loss(labels, predictions > 0.5)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uvsXE9Aq28F"
      },
      "source": [
        "# **SETTING UP TRAINING ARGUMENT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P8uH2bWq5xK"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainerCallback\n",
        "\n",
        "class EvalCallback(TrainerCallback):\n",
        "    def on_epoch_end(self, args, state, control, model=None, **kwargs):\n",
        "        # Manual evaluation after each epoch\n",
        "        trainer.evaluate()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['validation'],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EvalCallback]  # Add the callback here\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y0qK3uLrL7d"
      },
      "source": [
        "# **INITIALIZE TRAINER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qX_G86WMrPCT"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['validation'],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVezVrMmrt38"
      },
      "source": [
        "# **TRAINING MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgTH_ohLuzq4"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcNlXtg0toeG"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load dataset\n",
        "dataset = load_dataset(\"go_emotions\")\n",
        "\n",
        "# 2. Initialize tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 3. Tokenize function\n",
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
        "\n",
        "# 4. Convert labels to multi-hot vectors (and ensure float32 now)\n",
        "num_labels = 28  # 27 emotions + neutral\n",
        "\n",
        "def encode_labels(example):\n",
        "    multi_hot = np.zeros(num_labels, dtype=np.float32)  # ensures float32\n",
        "    for label in example['labels']:\n",
        "        multi_hot[label] = 1.0\n",
        "    example['labels'] = multi_hot.astype(np.float32)  # enforce float32 explicitly\n",
        "    return example\n",
        "\n",
        "encoded_dataset = tokenized_dataset.map(encode_labels)\n",
        "\n",
        "# 5. Set PyTorch format AFTER casting\n",
        "encoded_dataset.set_format(\n",
        "    type='torch',\n",
        "    columns=['input_ids', 'attention_mask', 'labels'],\n",
        "    output_all_columns=False\n",
        ")\n",
        "\n",
        "# 5. Set PyTorch format\n",
        "encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5dqbkX48DDm"
      },
      "outputs": [],
      "source": [
        "print(encoded_dataset[\"train\"][0][\"labels\"].dtype)  # should print torch.float32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdWKNAmaw_Fc"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels,\n",
        "    problem_type=\"multi_label_classification\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoC6SPGVxA7U"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_dataset['train'],\n",
        "    eval_dataset=encoded_dataset['validation'],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics  # Define your metric functions\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c4WOmQTvwDL"
      },
      "outputs": [],
      "source": [
        "train_data = encoded_dataset[\"train\"].select(range(500))\n",
        "eval_data = encoded_dataset[\"validation\"].select(range(100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DonU-0V59k7M"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataloader import default_collate\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = default_collate(batch)\n",
        "    batch['labels'] = batch['labels'].float()  # force float tensor\n",
        "    return batch\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=eval_data,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGhylbImrxdY"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0lydg3Vr0mU"
      },
      "source": [
        "# **EVALUATE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Pmihgq_sd13"
      },
      "outputs": [],
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlpudXWWsZxs"
      },
      "source": [
        "# **TEST ON REAL WORLD TEXT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGT6ZhtRsj-e"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample texts\n",
        "sample_texts = [\n",
        "    \"I'm so happy and excited about the new opportunity!\",\n",
        "    \"This is absolutely terrible and I feel hopeless.\",\n",
        "    \"I don't know how to feel, it's all so confusing.\"\n",
        "]\n",
        "\n",
        "# Load pre-trained GoEmotions model\n",
        "model_name = \"monologg/bert-base-cased-goemotions-original\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Label list (43 emotions + neutral for this model)\n",
        "label_list = model.config.id2label.values() if hasattr(model.config, 'id2label') else list(range(model.config.num_labels))\n",
        "\n",
        "# Tokenize\n",
        "inputs = tokenizer(sample_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Predict\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    probs = torch.sigmoid(outputs.logits)\n",
        "\n",
        "# Threshold for emotion detection\n",
        "threshold = 0.2\n",
        "preds = (probs >= threshold).int().numpy()\n",
        "\n",
        "# Show predictions and probabilities\n",
        "for i, (text, prob, pred) in enumerate(zip(sample_texts, probs.numpy(), preds)):\n",
        "    print(f\"\\nText: {text}\")\n",
        "\n",
        "    # Print emotions with prob > 0.1\n",
        "    print(\"Probabilities (top 5):\")\n",
        "    top_indices = prob.argsort()[-5:][::-1]\n",
        "    for idx in top_indices:\n",
        "        print(f\"{list(label_list)[idx]}: {prob[idx]:.2f}\")\n",
        "\n",
        "    # Predicted emotions\n",
        "    emotions = [list(label_list)[i] for i, val in enumerate(pred) if val == 1]\n",
        "    print(f\"Predicted Emotions (threshold {threshold}): {emotions}\")\n",
        "\n",
        "    # Optional: Plot emotion probabilities\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.bar([list(label_list)[i] for i in top_indices], [prob[i] for i in top_indices], color='skyblue')\n",
        "    plt.title(f\"Emotion Probabilities for: \\\"{text}\\\"\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}